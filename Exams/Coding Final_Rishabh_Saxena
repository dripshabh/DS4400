{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMvST+WuKWTauMjITTqtZKp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","from numpy import ndarray\n","from numpy import genfromtxt\n","import matplotlib.pyplot as plt\n","import sklearn\n","from scipy.stats import gaussian_kde\n","from scipy.integrate import quad, fixed_quad\n","from sklearn.model_selection import train_test_split\n","from sklearn.manifold import TSNE\n","from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler"],"metadata":{"id":"tLxEL7-_ghqf","executionInfo":{"status":"ok","timestamp":1744775066665,"user_tz":240,"elapsed":3,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":["#Q1"],"metadata":{"id":"QFLhuHXVpJHw"}},{"cell_type":"code","execution_count":45,"metadata":{"id":"ZnILkyEORnrT","executionInfo":{"status":"ok","timestamp":1744775066669,"user_tz":240,"elapsed":2,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"outputs":[],"source":["A = genfromtxt('life_expectancy_X.csv', delimiter=',')\n","y = genfromtxt('life_expectancy_y.csv', delimiter=',')\n","A_test = genfromtxt('life_expectancy_X_test.csv', delimiter=',')\n","y_test = genfromtxt('life_expectancy_y_test.csv', delimiter=',').reshape(-1, 1)\n","# scale data between 0 and 1\n","scaler = MinMaxScaler()\n","scaler.fit(A)\n","A = scaler.transform(A)\n","A_test = scaler.transform(A_test)\n","\n","def Lasso(w, phi, y, lmbda):\n","  return 1/n * np.linalg.norm(phi.dot(w) - y)**2 + lmbda * np.linalg.norm(w, ord=1)"]},{"cell_type":"code","source":["# implement GD algorithm to minimize LASSO\n","# define derivative of LASSO objective function\n","def lasso_deriv(w, X, y, hyper_param):\n","    n = X.shape[0]\n","    return (2 / n) * X.T.dot(X.dot(w) - y) + (hyper_param * np.sign(w))\n","\n","# define lasso gradient descent function\n","def lasso_gd(X, y, hyper_param, step, iter):\n","    # add bias term\n","    X = np.hstack((X, np.ones((X.shape[0], 1))))\n","    # initialize w\n","    n = X.shape[1]\n","    w = np.ones((n, 1))\n","\n","    # step through\n","    for _ in range(iter):\n","        w = w - (step*lasso_deriv(w, X, y, hyper_param))\n","\n","    return w"],"metadata":{"id":"DBHoWOV6hYOQ","executionInfo":{"status":"ok","timestamp":1744775066710,"user_tz":240,"elapsed":41,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["# define a function that predicts y\n","def pred(X, w):\n","    # add bias term\n","    X = np.hstack((X, np.ones((X.shape[0], 1))))\n","    return X.dot(w)\n","\n","# define a function that gets mse\n","def mse(y, y_pred):\n","    return np.mean((y - y_pred)**2)\n"],"metadata":{"id":"R_0e0PfRjMgj","executionInfo":{"status":"ok","timestamp":1744775066714,"user_tz":240,"elapsed":2,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["# testing my LASSO GD with 0.1 lambda value\n","w = lasso_gd(A, y, 0.1, 0.001, 100000)\n","\n","print('My Lasso Regression MSE on test set:', mse(y_test, pred(A_test, w)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":324},"id":"81BHk220sotr","executionInfo":{"status":"error","timestamp":1744775585947,"user_tz":240,"elapsed":519234,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}},"outputId":"362c8f82-efd7-4c86-9279-b3d6b8040f81"},"execution_count":48,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-ad67f821b612>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# testing my LASSO GD with 0.1 lambda value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlasso_gd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'My Lasso Regression MSE on test set:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-46-c4eae33d84c1>\u001b[0m in \u001b[0;36mlasso_gd\u001b[0;34m(X, y, hyper_param, step, iter)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# step through\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlasso_deriv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-46-c4eae33d84c1>\u001b[0m in \u001b[0;36mlasso_deriv\u001b[0;34m(w, X, y, hyper_param)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlasso_deriv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhyper_param\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# define lasso gradient descent function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# find best lambda value\n","# initialize best lambda and mse values\n","best_lambda = 0\n","best_mse = 100000\n","for i in range(-6, 3):\n","    # check for hyperparameter = 0 as well\n","    if i == 2:\n","        alpha = 0\n","    else:\n","        alpha = 10**i\n","\n","    # fit to X and predict and get MSE on X_test\n","    w = lasso_gd(A, y, alpha, 0.001, 100000)\n","    pred_test = pred(A_test, w)\n","    get_mse = mse(y_test, pred_test)\n","\n","    # print to compare mse's\n","    print('mse:', get_mse, 'for lambda:', alpha)\n","    if get_mse < best_mse:\n","        # update best mse\n","        best_mse = get_mse\n","        # update best lambda\n","        best_lambda = alpha\n","print('\\nBEST MSE:', best_mse, 'FOR LAMBDA:', best_lambda)"],"metadata":{"id":"yQJnOE88mDvk","executionInfo":{"status":"aborted","timestamp":1744775585935,"user_tz":240,"elapsed":519341,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# using best lambda, get best w\n","w = lasso_gd(A, y, best_lambda, 0.01, 10000)\n","\n","print('Best w:', w)"],"metadata":{"id":"bwRl3aMolM-u","executionInfo":{"status":"aborted","timestamp":1744775585939,"user_tz":240,"elapsed":519341,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["QUESTION 2\n","\n","The factors that most positively influence longevity are: Exercise Amount, Amount of Supportive Relationships, Number of Siblings.\n","\n","The factors that negatively impact longevity are: Drug Consumption"],"metadata":{"id":"WnXqlyJOpEe5"}},{"cell_type":"code","source":["#QUESTION 3\n","def eigh_sort(Q):\n","  [D,V] = np.linalg.eigh(Q)\n","  idx = D.argsort()[::-1]\n","  D = D[idx]\n","  V = V[:,idx]\n","  return [D,V]\n","def centering_matrix(X):\n","  n = X.shape[0]\n","  return np.identity(n) - 1/n*np.ones_like(np.identity(n))\n","C = centering_matrix(A)\n","Q = A.T.dot(C).dot(A)\n","[D,V] = eigh_sort(Q)\n","#get first four eigenvectors\n","V = V[:,:4] # stacked columnwise >:|\n","A_hat = A.dot(V)\n","w_after_gradient_descent = gradient_descent(A_hat, y, lmbda = 0)\n","print(w_after_gradient_descent)\n"],"metadata":{"id":"Gij_8nuGo6Gw","executionInfo":{"status":"aborted","timestamp":1744775585941,"user_tz":240,"elapsed":519341,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["QUESTION 4\n","PCA may not be a good idea for this problem, because the transformed data is not immediately correlated to a meaning in the data. However it may be good to get rid of unneccessary dimensionality (since 3 of the dimensions seem to have no variance on the data.)"],"metadata":{"id":"Z_7jzOUWxf6T"}},{"cell_type":"code","source":["def correlate_all_dimensions(X, X_hat):\n","    matrix_result = np.zeros((X.shape[1], X_hat.shape[1]))\n","    for i in range(X.shape[1]):\n","        for j in range(X_hat.shape[1]):\n","            x = X[:, i]\n","            y = X_hat[:, j]\n","            corr = np.corrcoef(x, y)[0, 1]\n","            matrix_result[i, j] = corr\n","    return matrix_result\n","result = correlate_all_dimensions(A, A_hat)\n","print('TABLE OF CORRELATIONS:')\n","for meaning, row in zip(w_meaning, result):\n","    formatted_row = '  '.join(f\"{val:>8.4f}\" for val in row)\n","    print(f\"{meaning:<40}: {formatted_row}\")\n"],"metadata":{"id":"rMIfvlOqxT-N","executionInfo":{"status":"aborted","timestamp":1744775585942,"user_tz":240,"elapsed":519340,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As seen in the table above, The Exercise Amount label is somewhat correlated to feature 3. The Supportive Relationships label is highly correlated with the feature 2. The Number of Siblings label is highly negatively correlated with the feature 1. Drug Consumption label is highly correlated with feature 4."],"metadata":{"id":"VOBtJFtn3Utq"}},{"cell_type":"markdown","source":["The most correlated features mostly match the most important features in PCA, but they do not match for feature 3 which is less correlated than geature 4 even though it's more important.\n","\n","The most correlated features mostly match important features in the LASSO regression, but do not match for Exercise Amount which is extremely important, but is less correlated than every other label."],"metadata":{"id":"503jC03S5Bua"}},{"cell_type":"markdown","source":["#Q2"],"metadata":{"id":"gv9rmRPFFpaK"}},{"cell_type":"code","source":["# load data\n","A = genfromtxt('Question-2/gpa_prediction_X.csv', delimiter=',')\n","y = genfromtxt('Question-2/gpa_prediction_y.csv', delimiter=',')\n","A_test = genfromtxt('Question-2/gpa_prediction_X_test.csv', delimiter=',')\n","y_test = genfromtxt('Question-2/gpa_prediction_y_test.csv', delimiter=',')\n","\n","# scale data between 0 and 1\n","scaler = MinMaxScaler()\n","scaler.fit(A)\n","A = scaler.transform(A)\n","A_test = scaler.transform(A_test)\n","\n","def q(x=None, X=None, w=None):\n","\tif X is None:\n","\t\treturn 1/(1 + np.exp(-x.T.dot(w)))\n","\telse:\n","\t\treturn 1/(1 + np.exp(-X.dot(w)))\n","def dL_2(phi, y, w, lmda = 0.001):\n","\treturn phi.T.dot(q(None, phi, w) - y)/n + lmda * np.sign(w)\n","#QUESTION 1\n","def gradient_descent(phi, y, eta = 0.01):\n","  w = np.zeros(phi.shape[1])\n","  for i in range(10000):\n","    w = w - eta*dL_2(phi, y, w)\n","  return w\n","w_after_gradient_descent = gradient_descent(phi, y)\n","#report classification accuracy\n","def classification_accuracy(phi, y, w):\n","  predicted_y = q(None, phi, w)\n","  predicted_y[predicted_y >= 0.5] = 1\n","  predicted_y[predicted_y < 0.5] = 0\n","  return np.mean(predicted_y == y)\n","accuracy = classification_accuracy(phi, y, w_after_gradient_descent)\n","w_meaning = ['Exercise amount', 'Amount of Supportive Relationships', 'Number of siblings', 'Drug consumption', 'Height', 'Attractiveness', 'Work Ethic']\n","for meaning, weight in zip(w_meaning, w_after_gradient_descent):\n","    print(f\"{meaning}: {weight:.4f}\")\n","A_test = genfromtxt('gpa_prediction_X_test.csv', delimiter=',')\n","y_test = genfromtxt('gpa_prediction_y_test.csv', delimiter=',')\n","A_test = MinMaxScaler().fit_transform(A_test)\n","phi_test = np.hstack((np.ones((A_test.shape[0],1)),A_test))\n","accuracy = classification_accuracy(phi_test, y_test, w_after_gradient_descent)\n","print ('The accuracy of the classification algorithm on the test set was', accuracy)"],"metadata":{"id":"Em6XiGxHFqoW","executionInfo":{"status":"aborted","timestamp":1744775585943,"user_tz":240,"elapsed":519340,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["According to the logistic regression weights, Height is very negatively influential on GPA, while Number of siblings is the feature that is most positively influential followed by exercise, work ethic, supportive relationships and drug consumption on GPA."],"metadata":{"id":"jTRQM2tAjHXn"}},{"cell_type":"code","source":["#QUESTION 3\n","def eigh_sort(Q):\n","  [D,V] = np.linalg.eigh(Q)\n","  idx = D.argsort()[::-1]\n","  D = D[idx]\n","  V = V[:,idx]\n","  return [D,V]\n","def centering_matrix(X):\n","  n = X.shape[0]\n","  return np.identity(n) - 1/n*np.ones_like(np.identity(n))\n","def SB_matrix(X,y):\n","  X0=X[y==0]\n","  X1=X[y==1]\n","  return 1/X0.shape[0]*X0.T.dot(X0) - 1/X1.shape[0]*X1.T.dot(X1)\n","def SW_matrix(X,y):\n","  X0=X[y==0]\n","  X1=X[y==1]\n","  C0 = centering_matrix(X0)\n","  C1 = centering_matrix(X1)\n","  return X0.T.dot(C0).dot(X0) + X1.T.dot(C1).dot(X1)\n","SB = SB_matrix(A,y)\n","SW = SW_matrix(A,y)\n","Q = np.linalg.inv(SW).dot(SB)\n","[D,V] = eigh_sort(Q)\n","#get first four eigenvectors\n","V = V[:,:1] # stacked columnwise >:|\n","A_hat = A.dot(V)\n","A_hat = np.hstack((np.ones((A_hat.shape[0],1)),A_hat))\n","w_after_gradient_descent = gradient_descent(A_hat, y)\n","accuracy = classification_accuracy(A_hat, y, w_after_gradient_descent)\n","print ('The accuracy of the classification algorithm after LDA was', accuracy)\n"],"metadata":{"id":"3qO7YF_Kj2SA","executionInfo":{"status":"aborted","timestamp":1744775585944,"user_tz":240,"elapsed":519340,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Test\n","SB_test = SB_matrix(A_test,y_test)\n","SW_test = SW_matrix(A_test,y_test)\n","Q_test = np.linalg.inv(SW_test).dot(SB_test)\n","[D,V] = eigh_sort(Q_test)\n","#get first four eigenvectors\n","V = V[:,:2] # stacked columnwise >:|\n","A_hat = A.dot(V)\n","A_hat = np.hstack((np.ones((A_hat.shape[0],1)),A_hat))\n","w_after_gradient_descent = gradient_descent(A_hat, y)\n","accuracy = classification_accuracy(A_hat, y, w_after_gradient_descent)\n","print ('The accuracy of the classification algorithm on the test set after LDA was', accuracy)"],"metadata":{"id":"ThUpngoSuGd1","executionInfo":{"status":"aborted","timestamp":1744775585950,"user_tz":240,"elapsed":519346,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def correlate_all_dimensions(X, X_hat):\n","    matrix_result = np.zeros((X.shape[1], X_hat.shape[1]))\n","    for i in range(X.shape[1]):\n","        for j in range(X_hat.shape[1]):\n","            x = X[:, i]\n","            y = X_hat[:, j]\n","            corr = np.corrcoef(x, y)[0, 1]\n","            matrix_result[i, j] = corr\n","    return matrix_result\n","result = correlate_all_dimensions(A, A_hat)\n","print('TABLE OF CORRELATIONS:')\n","for meaning, row in zip(w_meaning, result):\n","    formatted_row = '  '.join(f\"{val:>8.4f}\" for val in row)\n","    print(f\"{meaning:<40}: {formatted_row}\")\n"],"metadata":{"id":"soEyP2Fsqtqv","executionInfo":{"status":"aborted","timestamp":1744775585951,"user_tz":240,"elapsed":519346,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["QUESTION 4:\n","I did LDA down to two dimension, since it preserved a decent amount of classification accuracy and I was quite impressed. The factors that correlate (positively or negatively) the most with the singular feature are drug consumption, attractiveness, number of siblings, and exercise amount."],"metadata":{"id":"hvRlng3XxXlr"}},{"cell_type":"markdown","source":["QUESTION 5: The most important factors from the LDA(drug consumption attractiveness, siblings, and exercise amount) did not clearly correspond to the most important factors from just regression (height negatively and number of siblings positively)."],"metadata":{"id":"kYnbUhuixvqu"}},{"cell_type":"markdown","source":["#Q3"],"metadata":{"id":"TjXrjUWmiBUa"}},{"cell_type":"code","source":["X = genfromtxt(\"final_clustering.csv\", delimiter=',')\n","plt.scatter(X[:,0], X[:,1])"],"metadata":{"id":"puTEtafNxvNM","executionInfo":{"status":"aborted","timestamp":1744775585974,"user_tz":240,"elapsed":519369,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def Kmeans(X, k, max_iter=1000, tol=1e-4):\n","    n_samples, n_features = X.shape\n","    rng = np.random.default_rng()\n","    centroids = X[rng.choice(n_samples, size=k, replace=False)]\n","\n","    for _ in range(max_iter):\n","        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n","        labels = np.argmin(distances, axis=1)\n","\n","        new_centroids = np.zeros((k, n_features))\n","        for i in range(k):\n","            if np.any(labels == i):\n","                new_centroids[i] = X[labels == i].mean(axis=0)\n","            else:\n","                new_centroids[i] = centroids[i]\n","\n","        if np.all(np.linalg.norm(new_centroids - centroids, axis=1) < tol):\n","            break\n","\n","        centroids = new_centroids\n","\n","    return labels, centroids\n","\n","\n","def Hierarchical_clustering(X, k):\n","    # Start with each point in its own cluster\n","    n_samples = X.shape[0]\n","    clusters = [{i} for i in range(n_samples)]\n","\n","    # Distance matrix between all points\n","    dists = np.linalg.norm(X[:, np.newaxis] - X[np.newaxis, :], axis=2)\n","    np.fill_diagonal(dists, np.inf)  # avoid self-merging\n","\n","    while len(clusters) > k:\n","        # Find the two closest clusters\n","        min_dist = np.inf\n","        to_merge = (None, None)\n","        for i in range(len(clusters)):\n","            for j in range(i + 1, len(clusters)):\n","                # Average linkage (you could also do single or complete)\n","                indices_i = list(clusters[i])\n","                indices_j = list(clusters[j])\n","                dist = np.mean([dists[p1, p2] for p1 in indices_i for p2 in indices_j])\n","                if dist < min_dist:\n","                    min_dist = dist\n","                    to_merge = (i, j)\n","\n","        # Merge the clusters\n","        i, j = to_merge\n","        clusters[i] = clusters[i].union(clusters[j])\n","        del clusters[j]\n","\n","    # Build final labels\n","    labels = np.zeros(n_samples, dtype=int)\n","    for cluster_id, cluster in enumerate(clusters):\n","        for idx in cluster:\n","            labels[idx] = cluster_id\n","\n","    return labels\n","true_labels = genfromtxt(\"final_clustering_solution.csv\", delimiter=',')\n","hlabels = Hierarchical_clustering(X, 2)\n","plt.scatter(X[:,0], X[:,1], c=hlabels)\n","plt.title('Hierarchical Clustering')\n","plt.show()\n","klabels, centroids = Kmeans(X, 2)\n","plt.scatter(X[:,0], X[:,1], c=klabels)\n","plt.title('K-means Clustering')\n","plt.show()\n","\n","#calculate normalized mutual information\n","print('the normalized mutual information score for the labels is', sklearn.metrics.normalized_mutual_info_score(true_labels, hlabels))\n","\n"],"metadata":{"id":"_Uf4GHHlii0Q","executionInfo":{"status":"aborted","timestamp":1744775585976,"user_tz":240,"elapsed":519370,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Q4"],"metadata":{"id":"kMjHeYBU8Gv3"}},{"cell_type":"code","source":["#QUESTION 1\n","X = genfromtxt('parental_anxiety_vs_kids_age.csv', delimiter=',')\n","\n","n, bins, patches = plt.hist(X,20, facecolor = 'blue', density = True, alpha= 0.7)\n","stdev = 0.85\n","def p(x, stdev, X):\n","  def gaussian(μ, sigma, x): return np.exp(-0.5 * ((x - μ) / sigma)**2) / (sigma * np.sqrt(2*np.pi))\n","  px = 0\n","  for xi in X:\n","    px += gaussian(xi, stdev, x)\n","  return 1/X.shape[0] * px\n","\n","x = np.linspace(np.mean(X)-4*np.std(X), np.mean(X)+4*np.std(X), 100)\n","y = p(x, stdev, X)\n","plt.plot(x,y, linewidth=2, color='r')\n","plt.show()"],"metadata":{"id":"v9i9wdoyspaa","executionInfo":{"status":"aborted","timestamp":1744775585977,"user_tz":240,"elapsed":519371,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#QUESTION 2\n","#generate an array of randomly sampled X's\n","idxs = np.random.randint(low = 0, high = X.shape[0], size = 1000)\n","X_samples = X[idxs]\n","labels = p(X_samples, stdev, X)\n","#polynomial_regression\n","polynomialfeatures = PolynomialFeatures(degree=4)\n","X_poly = polynomialfeatures.fit_transform(X_samples.reshape(-1, 1))\n","\n","#Linear Regression closed form\n","def closed_form_find_w(phi, y):\n","  closed_form_w = np.linalg.inv((phi.T.dot(phi))).dot((phi.T).dot(y))\n","  return closed_form_w\n","w = closed_form_find_w(X_poly, labels)\n","def q(x,w):\n","  return x.dot(w)\n"],"metadata":{"id":"7736_zw0DPsz","executionInfo":{"status":"aborted","timestamp":1744775585978,"user_tz":240,"elapsed":519371,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.hist(X,20, facecolor = 'blue', density = True, alpha= 0.7)\n","x = np.linspace(np.mean(X)-4*np.std(X), np.mean(X)+4*np.std(X), 100)\n","y = p(x, stdev, X)\n","z = q(polynomialfeatures.transform(x.reshape(-1, 1)), w)\n","plt.plot(x,y, linewidth=2, color='r')\n","plt.plot(x,z, linewidth=2, color='b')\n","plt.show()"],"metadata":{"id":"IHAF_GdzE080","executionInfo":{"status":"aborted","timestamp":1744775585979,"user_tz":240,"elapsed":519372,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Q5"],"metadata":{"id":"DpIi8iWqe7qJ"}},{"cell_type":"code","source":["#QUESTION 1 AND 2\n","def sample(X, num_samples):\n","    idxs = np.random.randint(low=0, high=X.shape[0], size=num_samples)\n","    return X[idxs]\n","\n","def KL_function(p, q):\n","  return lambda x: p(x) * np.log(p(x) / q(x))\n","def KL_sampling(p, q):\n","  return lambda x: np.log(p(x) / q(x))\n","\n","def big_function(X):\n","    integration_record = []\n","    sampling_record = []\n","    p = gaussian_kde(X.T)  # KDE expects shape (n_dims, n_samples)\n","    for i in range(1, 101):\n","        samples = sample(X, i * 200)\n","        q = gaussian_kde(samples.T)\n","        kl_integrand = KL_function(p, q)\n","        kl_pq, _ = fixed_quad(kl_integrand, np.min(X), np.max(X), n=50)\n","        integration_record.append(kl_pq)\n","        kl_pq_sampling = np.mean(KL_sampling(p, q)(X))\n","        sampling_record.append(kl_pq_sampling)\n","\n","    return integration_record, sampling_record\n","\n","integration_info, sampling_info = big_function(X)\n","plt.plot(integration_info)\n","plt.title('KL Divergence for Integration')\n","plt.show()\n","plt.plot(sampling_info)\n","plt.title('KL Divergence for Sampling')\n","plt.show()"],"metadata":{"id":"FEyi448Be87I","executionInfo":{"status":"aborted","timestamp":1744775585981,"user_tz":240,"elapsed":519373,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#QUESTION 3\n","plt.hist(X,20, facecolor = 'blue', density = True, alpha= 0.7)\n","x = np.linspace(np.mean(X)-4*np.std(X), np.mean(X)+4*np.std(X), 100)\n","samples = sample(X,60*200)\n","y = gaussian_kde(X)\n","z = gaussian_kde(samples)\n","plt.plot(x,y(x), linewidth=2, color='r')\n","plt.plot(x,z(x), linewidth=2, color='b')\n","plt.show()"],"metadata":{"id":"Q9Oc9MMhmCSi","executionInfo":{"status":"aborted","timestamp":1744775585982,"user_tz":240,"elapsed":519373,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["QUESTION 4:\n","I do think this is much better than the regression version, since it maintains the properties of being a pdf."],"metadata":{"id":"Yu0XPXV7sbcg"}},{"cell_type":"markdown","source":["#Q6"],"metadata":{"id":"K5_nD3ztIzVO"}},{"cell_type":"code","source":["\n","A = genfromtxt('Pollution_data.csv', delimiter=',')\n","#QUESTION 1\n","def eigh_sort(Q):\n","  [D,V] = np.linalg.eigh(Q)\n","  idx = D.argsort()[::-1]\n","  D = D[idx]\n","  V = V[:,idx]\n","  return [D,V]\n","def centering_matrix(X):\n","  n = X.shape[0]\n","  return np.identity(n) - 1/n*np.ones_like(np.identity(n))\n","C = centering_matrix(A)\n","Q = A.T.dot(C).dot(A)\n","[D,V] = eigh_sort(Q)\n","V = V[:,:2] # stacked columnwise >:|\n","A_hat = A.dot(V)\n","print(D)\n"],"metadata":{"id":"4sp38Kqqsc8J","executionInfo":{"status":"aborted","timestamp":1744775585983,"user_tz":240,"elapsed":519373,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["All Eigenvalues except 2 are on the 1-e4 scale, so We likely only need to keep those 2 dimensions."],"metadata":{"id":"DWOULMcZ6Gh1"}},{"cell_type":"code","source":["#QUESTION 2\n","plt.scatter(A_hat[:,0], A_hat[:,1])"],"metadata":{"id":"SMXuEWGr7dZt","executionInfo":{"status":"aborted","timestamp":1744775585996,"user_tz":240,"elapsed":519385,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There is an obvious pattern between kids who were and weren't impacted by the pollution. There is a large group on the left and a large group on the right"],"metadata":{"id":"4KvmJdY77nNm"}},{"cell_type":"code","source":["#QUESTION 3\n","tsne = TSNE(n_components=2, random_state=42)\n","X_tsne = tsne.fit_transform(A)\n","\n","# Plot the 2D t-SNE result\n","plt.figure(figsize=(8, 6))\n","plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n","plt.title(\"t-SNE Projection (2D)\")\n","plt.xlabel(\"Component 1\")\n","plt.ylabel(\"Component 2\")\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"A3BjFV0C75RS","executionInfo":{"status":"aborted","timestamp":1744775585997,"user_tz":240,"elapsed":519385,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#calculate covariance matrix\n","group1 = A_hat[A_hat[:,0] > -15]\n","group2 = A_hat[A_hat[:,0] < -15] # looks like the higher variance\n","\n","group1_cov = np.cov(group1, rowvar=False)\n","group2_cov = np.cov(group2, rowvar=False)\n","print(group1_cov)\n","print(group2_cov)\n","#calculate and compare frobenius norms\n","print('group1:', np.linalg.norm(group1_cov, ord='fro'))\n","print('group2:', np.linalg.norm(group2_cov, ord='fro'))\n"],"metadata":{"id":"3f55N9wWA_uk","executionInfo":{"status":"aborted","timestamp":1744775585998,"user_tz":240,"elapsed":519384,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["QUESTION 4: We should use the PCA information likely, since it doesnt project into higher dimensions and add extra variance where there is none. It only smushes unimportant variance down."],"metadata":{"id":"Yv1A83w-DGoE"}},{"cell_type":"code","source":["#QUESTION 5\n","#logistic regression with train test splits\n","A = A_hat\n","#make our own labels\n","y = np.where(A[:,0] < -15,1,0)\n","X_train, X_rest, y_train, y_rest = train_test_split(A, y, test_size=0.2, random_state=42)\n","X_val, X_test, y_val, y_test = train_test_split(X_rest, y_rest, test_size=0.5, random_state=42)\n","phi_train = np.hstack((np.ones((X_train.shape[0],1)),X_train))\n","\n","def q(x=None, X=None, w=None):\n","\tif X is None:\n","\t\treturn 1/(1 + np.exp(-x.T.dot(w)))\n","\telse:\n","\t\treturn 1/(1 + np.exp(-X.dot(w)))\n","def dL_2(phi, y, w):\n","  n = phi.shape[0]\n","  return phi.T.dot(q(None, phi, w) - y)/n\n","\n","def gradient_descent(phi, y, eta = 0.01):\n","  w = np.zeros(phi.shape[1])\n","  for i in range(10000):\n","    w = w - eta*dL_2(phi, y, w)\n","  return w\n","w_after_gradient_descent = gradient_descent(phi_train, y_train)\n","#report classification accuracy\n","phi_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n","# make an f(x) that returns 1 or 0 when given a vector from X\n","def f(x=None, X=None):\n","  if X is None:\n","    return 1 if x.T.dot(w_after_gradient_descent) >= 0.5 else 0\n","  else:\n","    return np.array([1 if xi.T.dot(w_after_gradient_descent) >= 0 else 0 for xi in X])\n","def classification_accuracy(phi, y, w):\n","  predicted_y = f(None, phi)\n","  return np.mean(predicted_y == y)\n","accuracy = classification_accuracy(phi_test, y_test, w_after_gradient_descent)\n","print ('The accuracy of the classification algorithm on the test set was', accuracy)"],"metadata":{"id":"pPqicOJvDF6K","executionInfo":{"status":"aborted","timestamp":1744775585998,"user_tz":240,"elapsed":519383,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#QUESTION 6\n","# repeat classification with SVM\n","# create svm gradient descent function\n","def svm_gd(X, y, lr, num, l):\n","    m, n = X.shape\n","    # initialize weights\n","    w = np.ones(n + 1)\n","    # add bias term\n","    X = np.hstack((X, np.ones((m, 1))))\n","    # change labels to ±1\n","    y = np.where(y == 0, -1, 1)\n","\n","    for _ in range(num):\n","        for i, x_i in enumerate(X):\n","            check = y[i] * np.dot(x_i, w) >= 1\n","            if check:\n","                w -= lr * (2 * l * w)\n","            else:\n","                w -= lr * (2 * l * w - np.dot(x_i, y[i]))\n","    return w\n","\n","# define prediction function\n","def predict_svm(X, w):\n","    # add bias term\n","    X = np.hstack((X, np.ones((X.shape[0], 1))))\n","    linear = np.dot(X, w)\n","    # classify\n","    y_class = []\n","    for value in linear:\n","        if value >= 0:\n","            y_class.append(1)\n","        else:\n","            y_class.append(0)\n","    return y_class\n","\n","# testing accuracy on train and test\n","w_svm = svm_gd(X_train, y_train, 0.01, 1000, 0.01)\n","y_train_pred_svm = predict_svm(X_train, w_svm)\n","svm_train_accuracy = np.mean(y_train_pred_svm == y_train)\n","y_test_pred_svm = predict_svm(X_test, w_svm)\n","svm_test_accuracy = np.mean(y_test_pred_svm == y_test)\n","\n","# print answers\n","print(\"SVM Training Accuracy:\", svm_train_accuracy)\n","print(\"SVM Testing Accuracy:\", svm_test_accuracy)"],"metadata":{"id":"y3cTdZ_mI405","executionInfo":{"status":"aborted","timestamp":1744775585999,"user_tz":240,"elapsed":519384,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# validate results for logistic regression and SVM\n","# logistic regression validation accuracy\n","phi_val = np.hstack((np.ones((X_val.shape[0], 1)), X_val))\n","logistic_val_accuracy = classification_accuracy(phi_val, y_val, w_after_gradient_descent)\n","\n","# SVM validation accuracy\n","y_val_pred_svm = predict_svm(X_val, w_svm)\n","svm_val_accuracy = np.mean(y_val_pred_svm == y_val)\n","\n","# print accuracy scores\n","print(\"Logistic Regression Validation Accuracy:\", logistic_val_accuracy)\n","print(\"SVM Validation Accuracy:\", svm_val_accuracy)"],"metadata":{"id":"Cn-1v78aVMkY","executionInfo":{"status":"aborted","timestamp":1744775586000,"user_tz":240,"elapsed":519385,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Based on the results both sort the train, test and validation data with 100% accuracy."],"metadata":{"id":"Lt8bSQtRWFrs"}},{"cell_type":"code","source":["def Kmeans(X, k, max_iter=1000, tol=1e-4):\n","    n_samples, n_features = X.shape\n","    rng = np.random.default_rng()\n","    centroids = X[rng.choice(n_samples, size=k, replace=False)]\n","\n","    for _ in range(max_iter):\n","        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n","        labels = np.argmin(distances, axis=1)\n","\n","        new_centroids = np.zeros((k, n_features))\n","        for i in range(k):\n","            if np.any(labels == i):\n","                new_centroids[i] = X[labels == i].mean(axis=0)\n","            else:\n","                new_centroids[i] = centroids[i]\n","\n","        if np.all(np.linalg.norm(new_centroids - centroids, axis=1) < tol):\n","            break\n","\n","        centroids = new_centroids\n","\n","    return labels\n","\n"],"metadata":{"id":"pboJHoCFWJr0","executionInfo":{"status":"aborted","timestamp":1744775586001,"user_tz":240,"elapsed":519385,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# kmeans with 2 clusters (2 groups)\n","\n","y = np.where(A[:,0] < -15,1,0)\n","kmeans_labels = Kmeans(A, 2, 100)\n","\n","# check normalized mutual information\n","nmi = sklearn.metrics.normalized_mutual_info_score(kmeans_labels, y)\n","print(\"The Normalized Mutual Information is:\", nmi)"],"metadata":{"id":"PVp20-E9WTZD","executionInfo":{"status":"aborted","timestamp":1744775586001,"user_tz":240,"elapsed":519385,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOT68yotXICw8nTF+MdWgOk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"mUCItsg_ylro","executionInfo":{"status":"ok","timestamp":1737130519613,"user_tz":300,"elapsed":257,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}}},"outputs":[],"source":["import autograd.numpy as np\n","from autograd import grad"]},{"cell_type":"code","source":["#example code\n","def relu(w, x):\n","  v = np.dot(np.transpose(w),x)\n","  return np.maximum(0,v)\n","def grad_relu(w,x):\n","  if np.transpose(w).dot(x) > 0:\n","    return x\n","  else:\n","    return 0\n","for i in range(3):\n","  x = np.random.randn(2,1)\n","  w = np.random.randn(2,1)\n","\n","  grad_foo = grad(relu)\n","  print('w.dot(x) = %3.f'%x.T.dot((w)))\n","  print ('Autogen Gradient:', grad_foo(w,x), '\\n')\n","  print ('Theoretical gradient:', grad_relu(w,x), '\\n\\n')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f63ACA5QDA-k","executionInfo":{"status":"ok","timestamp":1737131009371,"user_tz":300,"elapsed":98,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}},"outputId":"1ad278ea-ff09-44e3-ec75-c992b1e7c1de"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["w.dot(x) =  -0\n","Autogen Gradient: [[0.]\n"," [0.]] \n","\n","Theoretical gradient: 0 \n","\n","\n","w.dot(x) =  -1\n","Autogen Gradient: [[0.]\n"," [0.]] \n","\n","Theoretical gradient: 0 \n","\n","\n","w.dot(x) =  -1\n","Autogen Gradient: [[0.]\n"," [0.]] \n","\n","Theoretical gradient: 0 \n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-7-559ddf16f190>:15: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n","  print('w.dot(x) = %3.f'%x.T.dot((w)))\n"]}]},{"cell_type":"code","source":["#QUESTION 1\n","def q1(x):\n","  y = np.array([[2],[5]])\n","  one = np.array([[1],[1]])\n","  return np.dot(np.transpose(y),x) + np.dot(np.transpose(x),one)\n","def grad_q1(x):\n","  return np.array([[2],[5]]) + np.array([[1],[1]])\n","for i in range(3):\n","  x = np.random.randn(2,1)\n","\n","  grad_foo = grad(q1)\n","  print('x = ', x)\n","  print ('Autogen Gradient:', grad_foo(x), '\\n')\n","  print ('Theoretical gradient:', grad_q1(x), '\\n\\n')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"loYvDAycDvW6","executionInfo":{"status":"ok","timestamp":1737131382059,"user_tz":300,"elapsed":83,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}},"outputId":"2d469b49-fbe8-4563-ea38-b695699fe316"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["x =  [[0.29361376]\n"," [1.08426278]]\n","Autogen Gradient: [[3.]\n"," [6.]] \n","\n","Theoretical gradient: [[3]\n"," [6]] \n","\n","\n","x =  [[-1.62697625]\n"," [ 0.51206551]]\n","Autogen Gradient: [[3.]\n"," [6.]] \n","\n","Theoretical gradient: [[3]\n"," [6]] \n","\n","\n","x =  [[-1.00843214]\n"," [ 0.39909927]]\n","Autogen Gradient: [[3.]\n"," [6.]] \n","\n","Theoretical gradient: [[3]\n"," [6]] \n","\n","\n"]}]},{"cell_type":"code","source":["#QUESTION 2\n","def q2(x):\n","  A = np.array([[2,1],[1,1]])\n","  return np.dot(np.transpose(x),np.dot(A,x))\n","def grad_q2(x):\n","  A = np.array([[2,1],[1,1]])\n","  return np.dot((np.transpose(A)+A),x)\n","for i in range(3):\n","  x = np.random.randn(2,1)\n","\n","  grad_foo = grad(q2)\n","  print('x = ', x)\n","  print ('Autogen Gradient:', grad_foo(x), '\\n')\n","  print ('Theoretical gradient:', grad_q2(x), '\\n\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZrD0qc5iGm8M","executionInfo":{"status":"ok","timestamp":1737131654375,"user_tz":300,"elapsed":85,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}},"outputId":"579b4e91-72d2-48f9-94a1-a459b40d39e4"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["x =  [[-2.04256118]\n"," [ 1.4572182 ]]\n","Autogen Gradient: [[-5.25580833]\n"," [-1.17068597]] \n","\n","Theoretical gradient: [[-5.25580833]\n"," [-1.17068597]] \n","\n","\n","x =  [[-0.84462443]\n"," [ 0.70787318]]\n","Autogen Gradient: [[-1.96275135]\n"," [-0.2735025 ]] \n","\n","Theoretical gradient: [[-1.96275135]\n"," [-0.2735025 ]] \n","\n","\n","x =  [[-0.16347602]\n"," [ 0.05631798]]\n","Autogen Gradient: [[-0.5412681 ]\n"," [-0.21431607]] \n","\n","Theoretical gradient: [[-0.5412681 ]\n"," [-0.21431607]] \n","\n","\n"]}]},{"cell_type":"code","source":["#QUESTION 3\n","def q3(x):\n","  A = np.array([[4,0],[1,1]])\n","  return np.exp(np.dot(np.transpose(x),np.dot(A,x)))\n","def grad_q3(x):\n","  A = np.array([[4,0],[1,1]])\n","  return q3(x) * (np.dot((np.transpose(A)+A),x))\n","for i in range(3):\n","  x = np.random.randn(2,1)\n","\n","  grad_foo = grad(q3)\n","  print('x = ', x)\n","  print ('Autogen Gradient:', grad_foo(x), '\\n')\n","  print ('Theoretical gradient:', grad_q3(x), '\\n\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y8lzZyX3HaI7","executionInfo":{"status":"ok","timestamp":1737131821537,"user_tz":300,"elapsed":117,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}},"outputId":"3ee8ff32-5525-466b-9762-c3995d44fbbe"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["x =  [[-0.49920692]\n"," [ 0.14265584]]\n","Autogen Gradient: [[-9.91747967]\n"," [-0.55084445]] \n","\n","Theoretical gradient: [[-9.91747967]\n"," [-0.55084445]] \n","\n","\n","x =  [[ 1.53041313]\n"," [-1.71413694]]\n","Autogen Gradient: [[169013.60640609]\n"," [-30464.352622  ]] \n","\n","Theoretical gradient: [[169013.60640609]\n"," [-30464.352622  ]] \n","\n","\n","x =  [[1.16499334]\n"," [0.33749727]]\n","Autogen Gradient: [[3654.27664858]\n"," [ 696.23232797]] \n","\n","Theoretical gradient: [[3654.27664858]\n"," [ 696.23232797]] \n","\n","\n"]}]},{"cell_type":"code","source":["#QUESTION 4\n","def q4(x):\n","  w = np.array([[7],[8]])\n","  return 1/(1+(np.exp(-1*np.dot(np.transpose(w),x))))\n","def grad_q4(x):\n","  w = np.array([[7],[8]])\n","  return -1 * 1/(1+(np.exp(-1*np.dot(np.transpose(w),x))))**2 * np.exp(-1*np.dot(np.transpose(w),x)) * -w\n","for i in range(3):\n","  x = np.random.randn(2,1)\n","\n","  grad_foo = grad(q4)\n","  print('x = ', x)\n","  print ('Autogen Gradient:', grad_foo(x), '\\n')\n","  print ('Theoretical gradient:', grad_q4(x), '\\n\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WCMxVnOWIDMT","executionInfo":{"status":"ok","timestamp":1737132194702,"user_tz":300,"elapsed":89,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}},"outputId":"27aadf5b-83df-4fbb-af80-526e383bd3bb"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["x =  [[-0.95078517]\n"," [ 1.26186722]]\n","Autogen Gradient: [[0.21083303]\n"," [0.24095203]] \n","\n","Theoretical gradient: [[0.21083303]\n"," [0.24095203]] \n","\n","\n","x =  [[1.09329307]\n"," [0.58954939]]\n","Autogen Gradient: [[2.97243219e-05]\n"," [3.39706536e-05]] \n","\n","Theoretical gradient: [[2.97243219e-05]\n"," [3.39706536e-05]] \n","\n","\n","x =  [[ 0.42316349]\n"," [-0.61665858]]\n","Autogen Gradient: [[0.75123073]\n"," [0.8585494 ]] \n","\n","Theoretical gradient: [[0.75123073]\n"," [0.8585494 ]] \n","\n","\n"]}]},{"cell_type":"code","source":["%%shell\n","jupyter nbconvert --to html /content/Lecture4.ipynb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l2Fwf9APJ7xY","executionInfo":{"status":"ok","timestamp":1737132352996,"user_tz":300,"elapsed":5139,"user":{"displayName":"Rishabh Saxena","userId":"01657484899872312362"}},"outputId":"7f88f46a-0bd2-43f7-a9b0-f033d9160e0d"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["[NbConvertApp] Converting notebook /content/Lecture4.ipynb to html\n","[NbConvertApp] Writing 294713 bytes to /content/Lecture4.html\n"]},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":18}]}]}